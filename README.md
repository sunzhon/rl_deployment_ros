 # Deploy the trained policy flexible mode

 ![deploy](./../docs/images/deploy2.png "a image to show the policy deployment")

### Environment setup
1. install necessary packages and libraries including:
  sudo apt-get install ros-$(rosversion -d)-ros-numpy
  pip install rospkg numpy


The input and output are depicted in the above figure, where $v_b$, $w_b$, $g_b$, and $c$ are the linear velocity, angular velocity, and gravity projected in the body frame. These variables are 3-dimensional vectors following x-y-z orders. $q$, $\dot{q}$ are the joint positions and joint velocities of 12 joints, respectively. The robot has four legs, each leg has three joints. The joints of a leg are ordered from upper to bottom, e.g., hip joint, knee joint, and ankle joint. The orders of the legs are left front leg, right front, left hind, and right hind leg. $a_{t-1}$ is the output of the neural network at the latest step, while the $a_t$ is the output of the neural network at the current step. 

### Normalization
To sacle the inputs of the network, a normalization module was used to scale the inputs. 
    `
    base_lin_vel * obs_scales["lin_vel"],
    base_ang_vel * obs_scales["ang_vel"],
    projected_gravity,
    commands * commands_scale,
    (dof_pos -default_dof_pos) * obs_scales["dof_pos"],
    dof_vel * obs_scales["dof_vel"],
    actions
    `
The normalization parameter values are as: 
    `
    lin_vel = 2
    ang_vel = 0.25
    dof_pos = 1.0
    dof_vel = 0.05
    `
default_dof_pos are default positions of the robot joints, which are [0,0,-1.57, 0, 0,1.57, 0,0,-1.57, 0,0,1.57].

### Scale
The actions outputed from the neural network have to be preprocessed before as desired joint position. In the preprocess, the actions are scaled by this function $\hat{a_t}=0.5*a_t+q_{def}$. $q_{def}$ represents the default_dof_pos.

**An example deployment code can be find at [test_policy.py](./../legged_gym/legged_gym/scripts/test_policy.py)**

```python
import torch
import os.path as osp
from legged_gym import LEGGED_GYM_ROOT_DIR

@torch.jit.script
def quat_rotate_inverse(q, v):
    shape = q.shape
    q_w = q[:, -1]
    q_vec = q[:, :3]
    a = v * (2.0 * q_w ** 2 - 1.0).unsqueeze(-1)
    b = torch.cross(q_vec, v, dim=-1) * q_w.unsqueeze(-1) * 2.0
    c = q_vec * torch.bmm(q_vec.view(shape[0], 1, 3), v.view(shape[0], 3, 1)).squeeze(-1) * 2.0
    return a - b + c


if __name__ == '__main__':
    # load model, please specify your model path correctly
    model_path = osp.join(LEGGED_GYM_ROOT_DIR,"logs","field_ambot","exported","policies","policy_1.pt")
    device ="cpu"
    model = torch.jit.load(model_path, map_location=torch.device(device))
    # parameters
    default_dof_pos = torch.tensor([0,0,-1.57,  0,0,1.57,  0,0,-1.57, 0,0,1.57]).to(device)
    obs_scales = {"lin_vel":2, "ang_vel": 0.25, "commands_scale": 2,"dof_pos": 1.0, "dof_vel": 0.05}
    actions = torch.zeros(1, 12, dtype=torch.float, device=device, requires_grad=False) 
    action_scale = 0.4

    # observation
    # please fill these varables during your usage
    base_lin_vel = torch.zeros(1,3,dtype=torch.float, device=device, requires_grad=False) # x, y, z, w
    base_ang_vel = torch.zeros(1,3,dtype=torch.float, device=device, requires_grad=False) # x, y, z, w
    base_quat = torch.tensor([0, 0, 0, 0.1],dtype=torch.float, device=device, requires_grad=False).unsqueeze(0) # x, y, z, w
    gravity_vec = torch.tensor([0,0,-1],dtype=torch.float,device=device,requires_grad=False).unsqueeze(0)
    projected_gravity = quat_rotate_inverse(base_quat, gravity_vec)
    dof_pos = torch.zeros(1, 12, dtype=torch.float, device=device, requires_grad=False) 
    dof_vel = torch.zeros(1, 12, dtype=torch.float, device=device, requires_grad=False) 
    commands = torch.zeros(1, 3, dtype=torch.float, device=device, requires_grad=False) # x vel, y vel, yaw vel, heading


    # normalization
    norm_obs = torch.cat(
            (base_lin_vel * obs_scales["lin_vel"],
            base_ang_vel  * obs_scales["ang_vel"],
            projected_gravity,
            commands * obs_scales["commands_scale"],
            (dof_pos - default_dof_pos) * obs_scales["dof_pos"],
            dof_vel * obs_scales["dof_vel"],
            actions
            ),dim=-1).to(device)
    # inference
    actions = model(norm_obs)

    # scale actions
    desired_dof_pos = action_scale * actions + default_dof_pos
    
    print("expected dof joint positin: {:}".format(desired_dof_pos))
```


$$

$$

# Deploy the model on your real robot using ambot_rlcontroller ros package
This version shows an example of how to deploy the model on the Ambot simulated robot or real robot (with Nvidia Jetson orin NX).
To deploy the trained model on Ambot, please copy the deployment in teh main control computer or a Jetson.

1. Install ROS and the [ambot_msgs](https://github.com/sunzhon/ambot_msgs.git) and [Joy](https://github.com/sunzhon/joy.git), and follow the instructions to set up the robot on branch `ambot_v1`

    Assuming the ros workspace is located in `${HOME}/workspace/ambot/AmbotRL`

2. Install pytorch on a Python 3 environment. (take the Python3 virtual environment on Nvidia Jetson NX as an example)
    ```bash
    sudo apt-get install python3-pip python3-dev python3-venv
    python3 -m venv ambot_rl_env
    source ambot_rl_env/bin/activate
    ```
    Download the pip wheel file from [here](https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048) with v1.10.0. Then install it with
    ```bash
    pip install torch-1.10.0-cp36-cp36m-linux_aarch64.whl
    ```

3. Install other dependencies and `rsl_rl`
    ```bash
    pip install numpy==1.16.4 numpy-ros
    pip install -e ./rsl_rl
    ```

4. In T_gru, run
    ```bash
    cd ${HOME}/workspace/ambot/
    source ./setup.sh
    source ambot_rl_env/bin/activate
    start_rl
    ```

â€‹    
